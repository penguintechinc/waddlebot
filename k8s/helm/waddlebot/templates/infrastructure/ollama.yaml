{{- if .Values.ollama.enabled }}
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: {{ .Values.ollama.persistence.name }}
  namespace: {{ .Values.namespace }}
  labels:
    app.kubernetes.io/name: ollama
    app.kubernetes.io/instance: {{ .Release.Name }}
    app.kubernetes.io/component: ai-inference
    app.kubernetes.io/managed-by: {{ .Release.Service }}
spec:
  accessModes:
    - {{ .Values.ollama.persistence.accessMode }}
  {{- if .Values.ollama.persistence.storageClass }}
  storageClassName: {{ .Values.ollama.persistence.storageClass }}
  {{- end }}
  resources:
    requests:
      storage: {{ .Values.ollama.persistence.size }}
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ollama
  namespace: {{ .Values.namespace }}
  labels:
    app.kubernetes.io/name: ollama
    app.kubernetes.io/instance: {{ .Release.Name }}
    app.kubernetes.io/component: ai-inference
    app.kubernetes.io/managed-by: {{ .Release.Service }}
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app.kubernetes.io/name: ollama
      app.kubernetes.io/instance: {{ .Release.Name }}
  template:
    metadata:
      labels:
        app.kubernetes.io/name: ollama
        app.kubernetes.io/instance: {{ .Release.Name }}
        app.kubernetes.io/component: ai-inference
    spec:
      containers:
      - name: ollama
        image: {{ .Values.ollama.image.repository }}:{{ .Values.ollama.image.tag }}
        imagePullPolicy: {{ .Values.ollama.image.pullPolicy }}
        ports:
        - name: http
          containerPort: 11434
          protocol: TCP
        env:
        - name: OLLAMA_HOST
          value: "0.0.0.0"
        {{- if .Values.ollama.extraEnv }}
        {{- range $key, $value := .Values.ollama.extraEnv }}
        - name: {{ $key }}
          value: {{ $value | quote }}
        {{- end }}
        {{- end }}
        volumeMounts:
        - name: ollama-data
          mountPath: /root/.ollama
        resources:
          requests:
            memory: {{ .Values.ollama.resources.requests.memory }}
            cpu: {{ .Values.ollama.resources.requests.cpu }}
            {{- if .Values.ollama.gpu.enabled }}
            nvidia.com/gpu: {{ .Values.ollama.gpu.count }}
            {{- end }}
          limits:
            memory: {{ .Values.ollama.resources.limits.memory }}
            cpu: {{ .Values.ollama.resources.limits.cpu }}
            {{- if .Values.ollama.gpu.enabled }}
            nvidia.com/gpu: {{ .Values.ollama.gpu.count }}
            {{- end }}
        livenessProbe:
          httpGet:
            path: /api/tags
            port: http
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /api/tags
            port: http
          initialDelaySeconds: 10
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
      volumes:
      - name: ollama-data
        persistentVolumeClaim:
          claimName: {{ .Values.ollama.persistence.name }}
      {{- if .Values.ollama.gpu.enabled }}
      nodeSelector:
        nvidia.com/gpu: "true"
      {{- end }}
---
apiVersion: v1
kind: Service
metadata:
  name: {{ .Values.ollama.service.name }}
  namespace: {{ .Values.namespace }}
  labels:
    app.kubernetes.io/name: ollama
    app.kubernetes.io/instance: {{ .Release.Name }}
    app.kubernetes.io/component: ai-inference
    app.kubernetes.io/managed-by: {{ .Release.Service }}
spec:
  type: {{ .Values.ollama.service.type }}
  ports:
  - port: {{ .Values.ollama.service.port }}
    targetPort: http
    protocol: TCP
    name: http
    {{- if and (eq .Values.ollama.service.type "NodePort") .Values.ollama.service.nodePort }}
    nodePort: {{ .Values.ollama.service.nodePort }}
    {{- end }}
  selector:
    app.kubernetes.io/name: ollama
    app.kubernetes.io/instance: {{ .Release.Name }}
{{- end }}
